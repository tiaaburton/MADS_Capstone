{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14fc4ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  7 of 7 completed\n",
      "[*********************100%***********************]  2 of 2 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  504 of 504 completed\n",
      "\n",
      "2 Failed downloads:\n",
      "- BF.B: No data found for this date range, symbol may be delisted\n",
      "- BRK.B: No data found, symbol may be delisted\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from datetime import datetime, date, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from fredapi import Fred\n",
    "from ta import add_all_ta_features\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from warnings import simplefilter\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def tech_analysis_spx():\n",
    "    data = yf.download(\n",
    "                # tickers list or string as well\n",
    "                tickers = '^GSPC',\n",
    "\n",
    "                # use \"period\" instead of start/end\n",
    "                # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
    "                # (optional, default is '1mo')\n",
    "                period = \"max\",\n",
    "\n",
    "                # fetch data by interval (including intraday if period < 60 days)\n",
    "                # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "                # (optional, default is '1d')\n",
    "                interval = \"1d\",\n",
    "\n",
    "                # group by ticker (to access via data['SPY'])\n",
    "                # (optional, default is 'column')\n",
    "                group_by = 'ticker',\n",
    "\n",
    "                # adjust all OHLC automatically\n",
    "                # (optional, default is False)\n",
    "                auto_adjust = True,\n",
    "\n",
    "                # download pre/post regular market hours data\n",
    "                # (optional, default is False)\n",
    "                prepost = True,\n",
    "\n",
    "                # use threads for mass downloading? (True/False/Integer)\n",
    "                # (optional, default is True)\n",
    "                threads = True,\n",
    "\n",
    "                # proxy URL scheme use use when downloading?\n",
    "                # (optional, default is None)\n",
    "                proxy = None\n",
    "            )\n",
    "\n",
    "    df = data.reindex(index=data.index[::-1])\n",
    "\n",
    "    df = add_all_ta_features(df, open=\"Open\", high=\"High\", low=\"Low\", close=\"Close\", volume=\"Volume\", fillna=True)\n",
    "\n",
    "    df = df.drop(['Open', 'High', 'Low', 'Close', 'Volume'], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def tech_analysis_10yr():\n",
    "    data = yf.download(\n",
    "                # tickers list or string as well\n",
    "                tickers = '^TNX',\n",
    "\n",
    "                # use \"period\" instead of start/end\n",
    "                # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
    "                # (optional, default is '1mo')\n",
    "                period = \"max\",\n",
    "\n",
    "                # fetch data by interval (including intraday if period < 60 days)\n",
    "                # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "                # (optional, default is '1d')\n",
    "                interval = \"1d\",\n",
    "\n",
    "                # group by ticker (to access via data['SPY'])\n",
    "                # (optional, default is 'column')\n",
    "                group_by = 'ticker',\n",
    "\n",
    "                # adjust all OHLC automatically\n",
    "                # (optional, default is False)\n",
    "                auto_adjust = True,\n",
    "\n",
    "                # download pre/post regular market hours data\n",
    "                # (optional, default is False)\n",
    "                prepost = True,\n",
    "\n",
    "                # use threads for mass downloading? (True/False/Integer)\n",
    "                # (optional, default is True)\n",
    "                threads = True,\n",
    "\n",
    "                # proxy URL scheme use use when downloading?\n",
    "                # (optional, default is None)\n",
    "                proxy = None\n",
    "            )\n",
    "\n",
    "    df = data.reindex(index=data.index[::-1])\n",
    "\n",
    "    df = add_all_ta_features(df, open=\"Open\", high=\"High\", low=\"Low\", close=\"Close\", volume=\"Volume\", fillna=True)\n",
    "\n",
    "    df = df.drop(['Open', 'High', 'Low', 'Close', 'Volume'], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def pull_tickers():\n",
    "    tickers = ['^VIX', '^VVIX', '^GSPC', '^SKEW', 'CL=F', 'HG=F', 'GC=F']\n",
    "    \n",
    "    df = yf.download(tickers=tickers,\n",
    "                # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
    "                # (optional, default is '1mo')\n",
    "                period = \"max\",\n",
    "\n",
    "                # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "                # (optional, default is '1d')\n",
    "                interval = \"1d\",\n",
    "\n",
    "                # group by ticker (to access via data['SPY'])\n",
    "                # (optional, default is 'column')\n",
    "                group_by = 'ticker',\n",
    "\n",
    "                # adjust all OHLC automatically\n",
    "                # (optional, default is False)\n",
    "                auto_adjust = True,\n",
    "\n",
    "                # download pre/post regular market hours data\n",
    "                # (optional, default is False)\n",
    "                prepost = True,\n",
    "\n",
    "                # use threads for mass downloading? (True/False/Integer)\n",
    "                # (optional, default is True)\n",
    "                threads = True,\n",
    "\n",
    "                # proxy URL scheme use use when downloading?\n",
    "                # (optional, default is None)\n",
    "                proxy = None\n",
    "            )\n",
    "\n",
    "    #df = data.reindex(index=data.index[::-1])\n",
    "    df = df.drop(['Open', 'High', 'Low', 'Volume'], axis=1, level=1)\n",
    "    df.columns = df.columns.map(' '.join).str.strip()\n",
    "    df.columns = [x.split()[0] for x in df.columns]\n",
    "    df = df.sort_index(axis=0, ascending=True)\n",
    "    df = df.fillna(method='ffill')\n",
    "    df = df.rename(columns={'^GSPC': 'SPX'})\n",
    "    \n",
    "    return df\n",
    "\n",
    "def RSI(ticker):\n",
    "    df = yf.download(tickers=ticker,\n",
    "                # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
    "                # (optional, default is '1mo')\n",
    "                period = \"max\",\n",
    "\n",
    "                # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "                # (optional, default is '1d')\n",
    "                interval = \"1d\",\n",
    "\n",
    "                # group by ticker (to access via data['SPY'])\n",
    "                # (optional, default is 'column')\n",
    "                group_by = 'ticker',\n",
    "\n",
    "                # adjust all OHLC automatically\n",
    "                # (optional, default is False)\n",
    "                auto_adjust = True,\n",
    "\n",
    "                # download pre/post regular market hours data\n",
    "                # (optional, default is False)\n",
    "                prepost = True,\n",
    "\n",
    "                # use threads for mass downloading? (True/False/Integer)\n",
    "                # (optional, default is True)\n",
    "                threads = True,\n",
    "\n",
    "                # proxy URL scheme use use when downloading?\n",
    "                # (optional, default is None)\n",
    "                proxy = None\n",
    "            )\n",
    "\n",
    "\n",
    "    df.columns = ['Open', 'High', 'Low', 'Adj Close', 'Volume']\n",
    "\n",
    "    ## 14_Day RSI\n",
    "    df['Up Move'] = np.nan\n",
    "    df['Down Move'] = np.nan\n",
    "    df['Average Up'] = np.nan\n",
    "    df['Average Down'] = np.nan\n",
    "    # Relative Strength\n",
    "    df['RS'] = np.nan\n",
    "    # Relative Strength Index\n",
    "    df['RSI'] = np.nan\n",
    "    ## Calculate Up Move & Down Move\n",
    "    for x in range(1, len(df)):\n",
    "        df['Up Move'][x] = 0\n",
    "        df['Down Move'][x] = 0\n",
    "\n",
    "        if df['Adj Close'][x] > df['Adj Close'][x-1]:\n",
    "            df['Up Move'][x] = df['Adj Close'][x] - df['Adj Close'][x-1]\n",
    "\n",
    "        if df['Adj Close'][x] < df['Adj Close'][x-1]:\n",
    "            df['Down Move'][x] = abs(df['Adj Close'][x] - df['Adj Close'][x-1])  \n",
    "\n",
    "    ## Calculate initial Average Up & Down, RS and RSI\n",
    "    df['Average Up'][14] = df['Up Move'][1:15].mean()\n",
    "    df['Average Down'][14] = df['Down Move'][1:15].mean()\n",
    "    df['RS'][14] = df['Average Up'][14] / df['Average Down'][14]\n",
    "    df['RSI'][14] = 100 - (100/(1+df['RS'][14]))\n",
    "    ## Calculate rest of Average Up, Average Down, RS, RSI\n",
    "    for x in range(15, len(df)):\n",
    "        df['Average Up'][x] = (df['Average Up'][x-1]*13+df['Up Move'][x])/14\n",
    "        df['Average Down'][x] = (df['Average Down'][x-1]*13+df['Down Move'][x])/14\n",
    "        df['RS'][x] = df['Average Up'][x] / df['Average Down'][x]\n",
    "        df['RSI'][x] = 100 - (100/(1+df['RS'][x]))\n",
    "\n",
    "    df['RSI SMA'] = df['RSI'].rolling(window=7).mean()\n",
    "    \n",
    "    return df['RSI SMA']\n",
    "\n",
    "def VVIX():\n",
    "    ticker_list = ['^VIX', '^VVIX']\n",
    "\n",
    "    data = yf.download(\n",
    "        # tickers list or string as well\n",
    "        tickers = ticker_list[0:],\n",
    "\n",
    "        # use \"period\" instead of start/end\n",
    "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
    "        # (optional, default is '1mo')\n",
    "        period = \"max\",\n",
    "\n",
    "        # fetch data by interval (including intraday if period < 60 days)\n",
    "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "        # (optional, default is '1d')\n",
    "        interval = \"1d\",\n",
    "\n",
    "        # group by ticker (to access via data['SPY'])\n",
    "        # (optional, default is 'column')\n",
    "        group_by = 'ticker',\n",
    "\n",
    "        # adjust all OHLC automatically\n",
    "        # (optional, default is False)\n",
    "        auto_adjust = True,\n",
    "\n",
    "        # download pre/post regular market hours data\n",
    "        # (optional, default is False)\n",
    "        prepost = True,\n",
    "\n",
    "        # use threads for mass downloading? (True/False/Integer)\n",
    "        # (optional, default is True)\n",
    "        threads = True,\n",
    "\n",
    "        # proxy URL scheme use use when downloading?\n",
    "        # (optional, default is None)\n",
    "        proxy = None\n",
    "    )\n",
    "\n",
    "    df = data.reindex(index=data.index[::-1])\n",
    "    df = df.drop(['Open', 'High', 'Low', 'Volume'], axis=1, level=1)\n",
    "    df.columns = df.columns.map(' '.join).str.strip()\n",
    "    df.columns = [x.split()[0] for x in df.columns]\n",
    "    df = df.sort_index(axis=0, ascending=True)\n",
    "    df = df.fillna(method='ffill')\n",
    "    df['vvix_norm'] = (df['^VVIX'] - df['^VVIX'].mean())/df['^VVIX'].std()\n",
    "    df['vix_norm'] = (df['^VIX'] - df['^VIX'].mean())/df['^VIX'].std()\n",
    "    df['vvix_vix_minus'] = df['vvix_norm'] - df['vix_norm']\n",
    "    df['vvix_vix_minus_norm'] = (df['vvix_vix_minus'] - df['vvix_vix_minus'].mean())/df['vvix_vix_minus'].std()\n",
    "    df['vvix_vix_minus_abs'] = abs(df['vvix_norm'] - df['vix_norm'])\n",
    "    df['vvix_vix_minus_abs_norm'] = (df['vvix_vix_minus_abs'] - df['vvix_vix_minus_abs'].mean())/df['vvix_vix_minus_abs'].std()\n",
    "    df['vvix_vix_div'] = df['vvix_norm'] / df['vix_norm']\n",
    "    df['vvix_vix_div_norm'] = (df['vvix_vix_div'] - df['vvix_vix_div'].mean())/df['vvix_vix_div'].std()\n",
    "    df['vix_diff_sma'] = df['vvix_vix_minus_abs_norm'].rolling(window=10).mean()\n",
    "    \n",
    "    return df['vix_diff_sma']\n",
    "\n",
    "def spx_corr():\n",
    "    url = r'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "    page = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    table = soup.find_all(\"table\")\n",
    "    ticker_df = pd.read_html(str(table[0]))\n",
    "    ticker_df = pd.DataFrame(ticker_df[0])\n",
    "    ticker_list = [x for x in ticker_df['Symbol']]\n",
    "    ticker_list.append('SPY')\n",
    "    data = yf.download(\n",
    "            # tickers list or string as well\n",
    "            tickers = ticker_list[0:],\n",
    "\n",
    "            # use \"period\" instead of start/end\n",
    "            # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
    "            # (optional, default is '1mo')\n",
    "            period = \"max\",\n",
    "\n",
    "            # fetch data by interval (including intraday if period < 60 days)\n",
    "            # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "            # (optional, default is '1d')\n",
    "            interval = \"1d\",\n",
    "\n",
    "            # group by ticker (to access via data['SPY'])\n",
    "            # (optional, default is 'column')\n",
    "            group_by = 'ticker',\n",
    "\n",
    "            # adjust all OHLC automatically\n",
    "            # (optional, default is False)\n",
    "            auto_adjust = True,\n",
    "\n",
    "            # download pre/post regular market hours data\n",
    "            # (optional, default is False)\n",
    "            prepost = True,\n",
    "\n",
    "            # use threads for mass downloading? (True/False/Integer)\n",
    "            # (optional, default is True)\n",
    "            threads = True,\n",
    "\n",
    "            # proxy URL scheme use use when downloading?\n",
    "            # (optional, default is None)\n",
    "            proxy = None\n",
    "        )\n",
    "\n",
    "    df = data.reindex(index=data.index[::-1])\n",
    "    df = df.drop(['Open', 'High', 'Low', 'Volume'], axis=1, level=1)\n",
    "    df.columns = df.columns.map(' '.join).str.strip()\n",
    "    df.columns = [x.split()[0] for x in df.columns]\n",
    "    df = df.sort_index(axis=0, ascending=True)\n",
    "\n",
    "    def calc_corr(df=df,days=20,span=500):\n",
    "        corr_dict = {}\n",
    "        for stock in df.columns:\n",
    "            for i in range(span):\n",
    "                corr_dict[stock] = df.loc[:, stock].rolling(window=days).corr(df['SPY'])\n",
    "        return corr_dict\n",
    "\n",
    "    dic_20 = calc_corr(days=20, span=1)\n",
    "\n",
    "    corr_df = pd.DataFrame(dic_20['ATVI'])\n",
    "    for stock in dic_20:\n",
    "        stock_df = pd.DataFrame(dic_20[stock])\n",
    "        corr_df = pd.merge(corr_df, stock_df, left_index=True, right_index=True)\n",
    "\n",
    "    corr_df.sort_index(ascending=False, inplace=True)    \n",
    "        \n",
    "    return corr_df.mean(axis=1)\n",
    "\n",
    "def fred_pull():\n",
    "    fred = Fred(api_key='')\n",
    "    fred_df = pd.DataFrame()\n",
    "    \n",
    "    #Inflation breakevens\n",
    "    fred_df['5yILBE'] = fred.get_series('T5YIE')\n",
    "    fred_df['5y5yILBE'] = fred.get_series('T5YIFR')\n",
    "    fred_df['10yILBE'] = fred.get_series('T10YIE')\n",
    "    \n",
    "    # Treasury rates\n",
    "    fred_df['TedSpread'] = fred.get_series('TEDRATE')\n",
    "    fred_df['FedFunds'] = fred.get_series('DFF')\n",
    "    fred_df['2yTreas'] = fred.get_series('DGS2')\n",
    "    fred_df['5yTreas'] = fred.get_series('DGS5')\n",
    "    fred_df['10yTreas'] = fred.get_series('DGS10')\n",
    "    fred_df['30yTreas'] = fred.get_series('DGS30')\n",
    "    fred_df['5yrReal'] = fred_df['5yTreas'] - fred_df['5yILBE']\n",
    "    fred_df['10yrReal'] = fred_df['10yTreas'] - fred_df['5yILBE']\n",
    "    fred_df['Repo'] = fred.get_series('RRPONTSYD')\n",
    "    \n",
    "    #Other\n",
    "    fred_df['WTI'] = fred.get_series('DCOILWTICO')\n",
    "    fred_df['USDGBP'] = fred.get_series('DEXUSUK')\n",
    "    fred_df['EURUSD'] = fred.get_series('DEXUSEU')\n",
    "    fred_df['USDYUAN'] = fred.get_series('DEXCHUS')\n",
    "    fred_df['USDYEN'] = fred.get_series('DEXJPUS')\n",
    "    fred_df['NFCI'] = fred.get_series('NFCI')\n",
    "    fred_df['NatGas'] = fred.get_series('DHHNGSP')\n",
    "    fred_df['Mortgage'] = fred.get_series('MORTGAGE30US')\n",
    "    fred_df['M1'] = fred.get_series('WM1NS')\n",
    "    fred_df['M2'] = fred.get_series('WM2NS')\n",
    "    fred_df['Desposits'] = fred.get_series('DPSACBW027SBOG')\n",
    "    fred_df['Demand Deposits'] = fred.get_series('WDDNS')\n",
    "    fred_df['C&I Loans'] = fred.get_series('TOTCI')\n",
    "    fred_df['UMICH Sentiment'] = fred.get_series('UMCSENT')\n",
    "    fred_df['UMICH Inflation'] = fred.get_series('MICH')\n",
    "    fred_df['JOLTS'] = fred.get_series('JTSJOL')\n",
    "    fred_df['Quits'] = fred.get_series('JTSQUR')\n",
    "    \n",
    "    # Curve feature engineering\n",
    "    fred_df['2s10s'] = fred_df['10yTreas'] - fred_df['2yTreas']\n",
    "    fred_df['2s30s'] = fred_df['30yTreas'] - fred_df['2yTreas']\n",
    "    fred_df['5s10s'] = fred_df['10yTreas'] - fred_df['5yTreas']\n",
    "    fred_df['10s30s'] = fred_df['30yTreas'] - fred_df['10yTreas']\n",
    "    fred_df['5s30s'] = fred_df['30yTreas'] - fred_df['5yTreas']\n",
    "    \n",
    "    #Econ indicators\n",
    "    fred_df['Labor_Force_Rate'] = fred.get_series('CIVPART')\n",
    "    fred_df['Unemployment'] = fred.get_series('UNRATE')\n",
    "    fred_df['Non-Farm Payrolls'] = fred.get_series('PAYEMS')\n",
    "    \n",
    "    fred_df = fred_df.fillna(method='ffill')\n",
    "    \n",
    "    return fred_df\n",
    "\n",
    "def build_df():\n",
    "    df = pull_tickers()\n",
    "    vix_df = VVIX()\n",
    "    df = pd.merge(df, vix_df, left_index=True, right_index=True, how='outer')\n",
    "    df['SPX_RSI'] = RSI('^GSPC')\n",
    "    df['SPX_20D_corr'] = spx_corr()\n",
    "    df['SPX_20D_corr'] = df['SPX_20D_corr'].clip(0,1)\n",
    "    df['SPX_20D_corr_delta'] = df['SPX_20D_corr'].diff(1)\n",
    "    fred_df = fred_pull()\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df.index = df.index.tz_localize(None)\n",
    "    fred_df.index = pd.to_datetime(fred_df.index)\n",
    "    fred_df.index = fred_df.index.tz_localize(None)\n",
    "    df = pd.merge(df, fred_df, left_index=True, right_index=True, how='outer')\n",
    "    tech_df = tech_analysis_spx()\n",
    "    tech_df.index = pd.to_datetime(tech_df.index)\n",
    "    tech_df.index = tech_df.index.tz_localize(None)\n",
    "    df = pd.merge(df, tech_df, left_index=True, right_index=True, how='outer')\n",
    "    tenyr_tech_df = tech_analysis_10yr()\n",
    "    tenyr_tech_df.index = pd.to_datetime(tenyr_tech_df.index)\n",
    "    tenyr_tech_df.index = tenyr_tech_df.index.tz_localize(None)\n",
    "    df = pd.merge(df, tenyr_tech_df, left_index=True, right_index=True, how='outer')\n",
    "    df = df.sort_index(ascending=False)\n",
    "    naaim = pd.read_excel('https://www.naaim.org/wp-content/uploads/2022/04/USE_Data-since-Inception_2022-4-06.xlsx').set_index('Date')\n",
    "    df = pd.merge(df, naaim, left_index=True, right_index=True, how='outer')\n",
    "    df = df.rename(columns={'Mean/Average': 'NAAIM'})\n",
    "    df = df.fillna(method='ffill')\n",
    "    df = df.sort_index(ascending=False)\n",
    "    df['SPX_1D'] = df['SPX'].diff(1)/df['SPX']\n",
    "    df['SPX_5D'] = df['SPX'].diff(5)/df['SPX']\n",
    "    df['SPX_10D'] = df['SPX'].diff(10)/df['SPX']\n",
    "    df['SPX_15D'] = df['SPX'].diff(15)/df['SPX']\n",
    "    df['SPX_20D'] = df['SPX'].diff(20)/df['SPX']\n",
    "    df['SPX_40D'] = df['SPX'].diff(40)/df['SPX']\n",
    "    df['SPX_60D'] = df['SPX'].diff(60)/df['SPX']\n",
    "    df['SPX_120D'] = df['SPX'].diff(120)/df['SPX']\n",
    "    \n",
    "    return df\n",
    "\n",
    "pull_df = build_df()\n",
    "\n",
    "windows = ['1d', '5d', 'SM', 'M']\n",
    "\n",
    "for window in windows:\n",
    "\n",
    "    df = pull_df.copy()\n",
    "    df = df.resample(window).mean().sort_index(ascending = False)\n",
    "\n",
    "    features = df.drop(['SPX_1D', 'SPX_5D', 'SPX_10D', 'SPX_15D', 'SPX_20D',\n",
    "                        'SPX_40D', 'SPX_60D', 'SPX_120D'], axis=1).columns\n",
    "\n",
    "    for feature in features:\n",
    "        df[feature+'_chg1'] = df[feature][::-1].diff(1)[::1]\n",
    "        df[feature+'_chg2'] = df[feature][::-1].diff(2)[::1]\n",
    "        df[feature+'_chg3'] = df[feature][::-1].diff(3)[::1]\n",
    "\n",
    "    df.to_csv(f'model_data_{window}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e43cd7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72393098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
